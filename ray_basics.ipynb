{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Implement a simple 2D-maze game in which a single player can move around in the four major directions.\n",
    "- Initialize the maze as a 5x5 grid to which player is confirmed\n",
    "- One the 25 grid cells in the \"goal\" that a player cell is called the \"seeker\" must reach\n",
    "- Instead of hard-coding the solution, employ a reinforcement learning algorithm, so that the seeker learns to find the goal\n",
    "- This is done by repeatedly running simulations of the maze, rewarding the seeker for finding the goal and keeping track of which decisions of the seeker worked and didn't\n",
    "- As running simulations can be parallelized and our RL algorithm can also be trained in parallel, we use the Ray API to parallelize the whole process"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Discrete:\n",
    "    def __init__(self, num_actions: int):\n",
    "        self.n = num_actions\n",
    "\n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 0=down, 1=left, 2=right, 3=up\n",
    "action_space = Discrete(4)\n",
    "print(action_space.sample())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "class Environment:\n",
    "    seeker, goal= (0,0), (4,4)\n",
    "    info = {\"seeker\":seeker , \"goal\":goal}\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.action_space = Discrete(4)\n",
    "        # 25 positions in the grid, hence 25 states in the observation space\n",
    "        self.observation_space = Discrete(5*5)\n",
    "\n",
    "    def reset(self):\n",
    "        self.seeker = (0,0)\n",
    "        self.goal = (4,4)\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "    def get_observation(self):\n",
    "        return 5 * self.seeker[0] + self.seeker[1]\n",
    "\n",
    "    def get_reward(self):\n",
    "        return 1 if self.seeker == self.goal else 0\n",
    "\n",
    "    def is_done(self):\n",
    "        return self.seeker == self.goal\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            # move down\n",
    "            self.seeker = (min(self.seeker[0] + 1, 4), self.seeker[1])\n",
    "        elif action == 1:\n",
    "            # move left\n",
    "            self.seeker = (self.seeker[0], max(self.seeker[1]-1, 0))\n",
    "        elif action == 2:\n",
    "            # move up\n",
    "            self.seeker = (max(self.seeker[0] - 1, 0), self.seeker[1])\n",
    "        elif action == 3:\n",
    "            # move right\n",
    "            self.seeker = (self.seeker[0], min(self.seeker[1]+1, 4))\n",
    "        else:\n",
    "            raise ValueError(\"invalid action\")\n",
    "\n",
    "        return self.get_observation(), self.get_reward(), self.is_done(), self.info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        os.system(\"cls\" if os.name==\"nt\" else \"clear\")\n",
    "        clear_output(wait=True)\n",
    "        grid = [[\"| \" for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = \"|G\"\n",
    "        grid[self.seeker[0]][self.seeker[1]] = \"|S\"\n",
    "        print(\"\".join([\"\".join(grid_row) for grid_row in grid]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |S|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "environment = Environment()\n",
    "\n",
    "while not environment.is_done():\n",
    "    random_action = environment.action_space.sample()\n",
    "    environment.step(random_action)\n",
    "    time.sleep(0.1)\n",
    "    environment.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, env):\n",
    "        self.state_action_table = [\n",
    "            [0 for _ in range(env.action_space.n)] for _ in range(env.observation_space.n)\n",
    "        ]\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def get_action(self, state, explore=True, epsilon=0.1):\n",
    "        if explore and random.uniform(0,1) < epsilon:\n",
    "            return self.action_space.sample()\n",
    "        return np.argmax(self.state_action_table[state])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Simulation(object):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def rollout(self, policy, render=False, explore=True, epsilon=0.1):\n",
    "        experiences = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy.get_action(state, explore, epsilon)\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            experiences.append([state, action, reward, next_state])\n",
    "            state = next_state\n",
    "\n",
    "            if render:\n",
    "                time.sleep(0.05)\n",
    "                self.env.render()\n",
    "\n",
    "        return experiences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |S|\n",
      "\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "untrained_policy = Policy(environment)\n",
    "sim = Simulation(environment)\n",
    "\n",
    "exp = sim.rollout(untrained_policy, render=True, epsilon=1.0)\n",
    "for row in untrained_policy.state_action_table:\n",
    "    print(row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def update_policy(policy, experiences):\n",
    "    alpha = 0.1\n",
    "    gamma = 0.6\n",
    "    for state, action, reward, next_state in experiences:\n",
    "        next_max = np.max(policy.state_action_table[next_state])\n",
    "        value = policy.state_action_table[state][action]\n",
    "        new_value = (1 - alpha) * value + alpha * (reward + gamma * next_max)\n",
    "        policy.state_action_table[state][action] = new_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Initialize policy and simulation\n",
    "- Run the simulation many times (i.e. 10000)\n",
    "- For each game, collect the experiences by running rollout\n",
    "- Then update policy on collected experiences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def train_policy(env, num_episodes=10000, weight=0.1, discount_factor=0.9):\n",
    "    policy = Policy(env)\n",
    "    sim = Simulation(env)\n",
    "    for i in range(num_episodes):\n",
    "        os.system(\"cls\" if os.name==\"nt\" else \"clear\")\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode {i}\")\n",
    "        experiences = sim.rollout(policy)\n",
    "        update_policy(policy, experiences)\n",
    "    return policy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9999\n"
     ]
    }
   ],
   "source": [
    "trained_policy = train_policy(environment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |S|\n",
      "\n",
      "8.0 steps on average for a total of 10 episodes.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=10):\n",
    "    \"\"\"Evaluate a trained policy through rollouts.\"\"\"\n",
    "    simulation = Simulation(env)\n",
    "    steps = 0\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        os.system(\"cls\" if os.name==\"nt\" else \"clear\")\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode {i}\")\n",
    "        experiences = simulation.rollout(policy, render=True, explore=False)\n",
    "        steps += len(experiences)\n",
    "\n",
    "    print(f\"{steps / num_episodes} steps on average \"\n",
    "          f\"for a total of {num_episodes} episodes.\")\n",
    "\n",
    "\n",
    "evaluate_policy(environment, trained_policy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[[0.027993599999999893,\n  0.016796159999982237,\n  0.016796159999999075,\n  0.017538973931170574],\n [0.03963142369601317,\n  0.016796159999895574,\n  0.007438860085394064,\n  0.0005441558191252189],\n [0.007097437421282746, 0, 0, 0],\n [0, 0, 0, 0],\n [0, 0, 0, 0],\n [0.04665599999999983,\n  0.0279935999999982,\n  0.016796159999949184,\n  0.046655983523067386],\n [0.07775999947179606,\n  0.027993518560430433,\n  0.007562327837657654,\n  0.017680383604204555],\n [0.07867258836218466, 0, 0, 0],\n [0, 0, 0, 0],\n [0, 0, 0, 0],\n [0.07775999999999973,\n  0.046655999999933015,\n  0.027993599999956088,\n  0.07775999996343501],\n [0.12959999999999955,\n  0.046624749716573044,\n  0.03145598478993046,\n  0.07792548994172488],\n [0.20848001895445598, 0.007619668707953201, 0, 0.009204991963268945],\n [0.19266739150389361, 0, 0, 0],\n [0, 0, 0, 0],\n [0.12959999999999955,\n  0.0777599999996754,\n  0.0466559999999485,\n  0.12959999999943222],\n [0.21599999999999944,\n  0.07086817931815799,\n  0.06797146952334471,\n  0.20580823617335486],\n [0.35999999999999904,\n  0.08869354322538635,\n  0.048768572560904964,\n  0.31200757842913274],\n [0.5999999999998832,\n  0.1380706876688603,\n  0.02882716324946082,\n  0.13222563180000002],\n [0.5695327900000001, 0.035999999999527924, 0, 0],\n [0.12959999999965602,\n  0.12959999999869967,\n  0.07775999999868694,\n  0.21599999999999944],\n [0.21599999999992953,\n  0.12959999999996172,\n  0.12959999999983393,\n  0.35999999999999904],\n [0.3599999999999985,\n  0.21599999999995817,\n  0.21599999999999317,\n  0.5999999999999991],\n [0.5999999999999759,\n  0.3599999999999832,\n  0.3599999999964262,\n  0.9999999999999994],\n [0, 0, 0, 0]]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_policy.state_action_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 16:39:40,350\tINFO services.py:1470 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"RAY_DISABLE_MEMORY_MONITOR\"] = \"1\"\n",
    "import ray\n",
    "\n",
    "ray.init()\n",
    "environment = Environment()\n",
    "env_ref = ray.put(environment)\n",
    "\n",
    "@ray.remote\n",
    "def create_policy():\n",
    "    env = ray.get(env_ref)\n",
    "    return Policy(env)\n",
    "\n",
    "@ray.remote\n",
    "class SimulationActor(Simulation):\n",
    "    def __init__(self):\n",
    "        env = ray.get(env_ref)\n",
    "        super().__init__(env)\n",
    "\n",
    "@ray.remote\n",
    "def update_policy_task(policy_ref, experiences_list):\n",
    "    [update_policy(policy_ref, ray.get(xp)) for xp in experiences_list]\n",
    "    return policy_ref\n",
    "\n",
    "def train_policy_parallel(num_episodes=10000, num_simulations=4):\n",
    "    policy = create_policy.remote()\n",
    "    simulations = [SimulationActor.remote() for _ in range(num_simulations)]\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = [sim.rollout.remote(policy) for sim in simulations]\n",
    "        policy = update_policy_task.remote(policy, experiences)\n",
    "\n",
    "    return ray.get(policy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |S|\n",
      "\n",
      "8.0 steps on average for a total of 10 episodes.\n"
     ]
    }
   ],
   "source": [
    "parallel_policy = train_policy_parallel()\n",
    "evaluate_policy(environment, parallel_policy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[[0.027993599999999893,\n  0.016796159999999928,\n  0.016796159999999928,\n  0.027993599999999893],\n [0.04665599999999983,\n  0.015917136942005056,\n  0.026148098019024697,\n  0.024515125524111975],\n [0.0644979254914315, 0, 0.0028434279334112592, 0],\n [0, 0, 0, 0],\n [0, 0, 0, 0],\n [0.04665599999999983,\n  0.027993599999999893,\n  0.016796159999999928,\n  0.04665599999999983],\n [0.07775999999999973,\n  0.027908408400389263,\n  0.027867459631817547,\n  0.07662763727417805],\n [0.1295108114618403, 0.004664233404552535, 0, 0.004675726524452857],\n [0.06010165876746837, 0, 0, 0],\n [0, 0, 0, 0],\n [0.07775999999999973,\n  0.04665599999999983,\n  0.027993599999999893,\n  0.07775999999999973],\n [0.12959999999999955,\n  0.04664856585172074,\n  0.04663696415623727,\n  0.12919285424593374],\n [0.21599998686151864,\n  0.04428686974497553,\n  0.01372677185309154,\n  0.047439534429514],\n [0.35005772270159574, 0, 0.0, 0],\n [0.08792045660134448, 0, 0, 0],\n [0.12959999999999955,\n  0.07775999999999973,\n  0.04665599999999983,\n  0.12959999999999955],\n [0.21599999999999944,\n  0.07775974891717974,\n  0.07775998617925352,\n  0.21599906085387036],\n [0.35999999999999904,\n  0.1285677500695981,\n  0.12569369966082272,\n  0.33891709444494567],\n [0.5999999999999991,\n  0.20859115828114436,\n  0.1761514371855935,\n  0.4245138252308806],\n [0.9202335569231275, 0, 0.0026849283018015604, 0],\n [0.12959999999999955,\n  0.12959999999999955,\n  0.07775999999999973,\n  0.21599999999999944],\n [0.21599999999999944,\n  0.12959999999999955,\n  0.12959999999999955,\n  0.35999999999999904],\n [0.35999999999999904,\n  0.21599999999999944,\n  0.21599999999999944,\n  0.5999999999999991],\n [0.5999999999999991,\n  0.35999999999999904,\n  0.35999999999999904,\n  0.9999999999999994],\n [0, 0, 0, 0]]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_policy.state_action_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}